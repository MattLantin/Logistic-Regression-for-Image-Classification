{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfa49fe1",
   "metadata": {},
   "source": [
    "# Part 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55e9f83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "#from bayes_opt import BayesianOptimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e1da6c",
   "metadata": {},
   "source": [
    "### Section 1: Logistic Regression for Digit Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de6f6af6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.9445833333333333\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset for training\n",
    "features_df = pd.read_csv(\"./data_trouser_dress/troudress_train_x.csv\").values\n",
    "labels = pd.read_csv(\"./data_trouser_dress/troudress_train_y.csv\").values\n",
    "test_features_df = pd.read_csv(\"./data_trouser_dress/troudress_test_x.csv\").values\n",
    "\n",
    "\n",
    "def apply_blur_to_images(images, image_shape):\n",
    "    from scipy.ndimage import convolve\n",
    "    # Define a 3x3 averaging filter kernel\n",
    "    kernel = np.ones((3, 3)) / 9.0\n",
    "    \n",
    "    blurred_images = np.empty_like(images)\n",
    "    for i, image in enumerate(images):\n",
    "        # Reshape the flattened image back to its 2D shape\n",
    "        image_2d = image.reshape(image_shape)\n",
    "        # Apply convolution using the averaging filter kernel\n",
    "        blurred_image = convolve(image_2d, kernel, mode='nearest')\n",
    "        # Flatten the blurred image back to its original shape\n",
    "        blurred_images[i] = blurred_image.flatten()\n",
    "        \n",
    "    return blurred_images\n",
    "\n",
    "# Assuming your images are 28x28 pixels, adjust if they're different\n",
    "image_shape = (28, 28)  # Update this according to your dataset\n",
    "\n",
    "# Apply noise reduction (blurring) to both training and test sets\n",
    "X_train_blurred = apply_blur_to_images(features_df, image_shape)\n",
    "X_test_blurred = apply_blur_to_images(test_features_df, image_shape)\n",
    "\n",
    "# The rest of your logistic regression workflow remains the same\n",
    "# For example:\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_blurred, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train, y_train.ravel())  # Ensure y_train is the correct shape\n",
    "\n",
    "# Predicting on validation set\n",
    "y_pred = model.predict(X_val)\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "print(f\"Validation Accuracy: {accuracy}\")\n",
    "\n",
    "# You can proceed with predicting on X_test_blurred and evaluating as needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903ad5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iters = range(0, 100)  # Testing from 10 to 100, in steps of 10\n",
    "\n",
    "# Re-initialize lists to store accuracies\n",
    "accuracies_untransformed = []\n",
    "accuracies_transformed = []\n",
    "\n",
    "for max_iter in max_iters:\n",
    "    # Model for untransformed features\n",
    "    model_untransformed = LogisticRegression(solver='liblinear', max_iter=max_iter)\n",
    "    model_untransformed.fit(features_df, labels.ravel())\n",
    "    acc_untransformed = accuracy_score(labels, model_untransformed.predict(features_df))\n",
    "    accuracies_untransformed.append(acc_untransformed)\n",
    "    \n",
    "    # Model for transformed features\n",
    "    model_transformed = LogisticRegression(solver='liblinear', max_iter=max_iter)\n",
    "    model_transformed.fit(X_train_blurred, labels.ravel())\n",
    "    acc_transformed = accuracy_score(labels, model_transformed.predict(X_train_blurred))\n",
    "    accuracies_transformed.append(acc_transformed)\n",
    "\n",
    "# Plotting the adjusted results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(max_iters, accuracies_untransformed, label='Model 0: Untransformed Features', marker='o')\n",
    "plt.plot(max_iters, accuracies_transformed, label='Model 1: Transformed Features', marker='x')\n",
    "plt.xlabel('Maximum Iterations')\n",
    "plt.ylabel('Accuracy on Training Data')\n",
    "plt.title('Effect of Max Iteration on Model Accuracy (Extended Range)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79be58dc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BayesianOptimization' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m pbounds \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m'\u001b[39m: (\u001b[38;5;241m0.001\u001b[39m, \u001b[38;5;241m10\u001b[39m)}\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Initialize Bayesian Optimization\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m opt \u001b[38;5;241m=\u001b[39m BayesianOptimization(\n\u001b[1;32m     13\u001b[0m     f\u001b[38;5;241m=\u001b[39mf_score,\n\u001b[1;32m     14\u001b[0m     pbounds\u001b[38;5;241m=\u001b[39mpbounds,\n\u001b[1;32m     15\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m     16\u001b[0m     random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     17\u001b[0m )\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Optimize\u001b[39;00m\n\u001b[1;32m     20\u001b[0m opt\u001b[38;5;241m.\u001b[39mmaximize(\n\u001b[1;32m     21\u001b[0m     init_points\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,  \u001b[38;5;66;03m# You can adjust this number based on how many initial points you want to explore\u001b[39;00m\n\u001b[1;32m     22\u001b[0m     n_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,  \u001b[38;5;66;03m# And adjust this for the number of iterations for the optimization process\u001b[39;00m\n\u001b[1;32m     23\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'BayesianOptimization' is not defined"
     ]
    }
   ],
   "source": [
    "# Define the function for Bayesian optimization\n",
    "def f_score(C):\n",
    "    lr = LogisticRegression(max_iter=500, C=C)\n",
    "    lr.fit(X_train, y_train)\n",
    "    acc = accuracy_score(y_test, lr.predict(X_test))\n",
    "    return acc\n",
    "\n",
    "# Define the bounds for the hyperparameter C\n",
    "pbounds = {'C': (0.001, 10)}\n",
    "\n",
    "# Initialize Bayesian Optimization\n",
    "opt = BayesianOptimization(\n",
    "    f=f_score,\n",
    "    pbounds=pbounds,\n",
    "    verbose=2,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "# Optimize\n",
    "opt.maximize(\n",
    "    init_points=2,  # You can adjust this number based on how many initial points you want to explore\n",
    "    n_iter=10,  # And adjust this for the number of iterations for the optimization process\n",
    ")\n",
    "\n",
    "C1 = opt.max['params'][\"C\"]     # Take the best parameters\n",
    "print(\"best C:\",C1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ebe606",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming the data loading and preprocessing steps have been done as per your initial code\n",
    "\n",
    "# Define a range of C values to explore. For example:\n",
    "c_values = np.logspace(-4, 4, 20)  # This creates 20 values spaced evenly on a log scale\n",
    "\n",
    "best_score = 0\n",
    "best_c = None\n",
    "\n",
    "# Make sure you are using the augmented and binarized data (X_train_augmented, y_train_augmented) for finding the best C\n",
    "# Assuming X_train, y_train are already defined and refer to the augmented, binarized dataset\n",
    "\n",
    "for c in c_values:\n",
    "    model = LogisticRegression(C=c, max_iter=500)  # Increased max_iter to ensure convergence\n",
    "    scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')\n",
    "    mean_score = np.mean(scores)\n",
    "    \n",
    "    if mean_score > best_score:\n",
    "        best_score = mean_score\n",
    "        best_c = c\n",
    "\n",
    "print(\"Best C:\", best_c)\n",
    "print(\"Best cross-validation accuracy:\", best_score)\n",
    "\n",
    "# After finding the best C, train the final model on the full training set\n",
    "final_model = LogisticRegression(C=best_c, max_iter=500)  # Using the best C value found\n",
    "final_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bcefafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training on the training set\n",
    "lr1 = LogisticRegression(max_iter=200,C=C1)\n",
    "lr1.fit(X_train,y_train)\n",
    "\n",
    "\n",
    "# Test accuracy in test set\n",
    "pre = lr1.predict(X_test)\n",
    "acc = accuracy_score(pre,y_test)\n",
    "print(\"Accuracy on the test set:\",acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742f8192",
   "metadata": {},
   "source": [
    "Next, the training data is binarized (turning each image into a black and white image), with the pixel value of each pixel less than or equal to 0.4 placed at 0 and greater than 0.4 placed at 1. Then, again, a Bayesian optimizer is used to select the best regularization parameter C for the logistic regression classifier to achieve the highest accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e618e6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_copy = X_train.copy()\n",
    "# Binarization of all training images\n",
    "for i in range(len(X_train_copy)):\n",
    "    for j in range(len(X_train_copy[i])):\n",
    "        if X_train_copy[i][j] <= 0.4:\n",
    "            X_train_copy[i][j] = 0\n",
    "        else:\n",
    "            X_train_copy[i][j] = 1\n",
    "X_train_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2141b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_copy = X_test.copy()\n",
    "# Binarization of all test images\n",
    "for i in range(len(X_test_copy)):\n",
    "    for j in range(len(X_test_copy[i])):\n",
    "        if X_test_copy[i][j] <= 0.4:\n",
    "            X_test_copy[i][j] = 0\n",
    "        else:\n",
    "            X_test_copy[i][j] = 1\n",
    "X_test_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3006563a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The optimal hyperparameter C, the regularization parameter, is selected using a Bayesian optimizer\n",
    "def f_score(C):\n",
    "    lr = LogisticRegression(max_iter=200,C=C)\n",
    "    lr.fit(X_train_copy,y_train)\n",
    "    acc = accuracy_score(y_test,lr.predict(X_test_copy))\n",
    "    return acc     # The goal of adjusting parameters to maximize accuracy\n",
    "\n",
    "\n",
    "# Determine the range of parameter values\n",
    "pbounds = {'C': (0,1)}\n",
    "\n",
    "# Constructing a Bayesian Optimizer\n",
    "opt = BayesianOptimization(\n",
    "    f=f_score,\n",
    "    pbounds=pbounds,\n",
    "    verbose=2,  # verbose = 2 prints all, verbose = 1 prints the maximum value found in the run, verbose = 0 prints nothing\n",
    "    random_state=1\n",
    ")\n",
    "\n",
    "# Start running\n",
    "opt.maximize(\n",
    "    init_points=10,  # Steps of random search\n",
    "    n_iter=130  # Number of iterations to perform Bayesian optimization\n",
    ")\n",
    "\n",
    "C2 = opt.max['params'][\"C\"]     # Take the best parameters\n",
    "print(\"best C:\",C2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97b923e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training on the training set\n",
    "lr2 = LogisticRegression(max_iter=200,C=C2)\n",
    "lr2.fit(X_train_copy,y_train)\n",
    "\n",
    "\n",
    "# Test accuracy in test set\n",
    "pre = lr2.predict(X_test_copy)\n",
    "acc = accuracy_score(pre,y_test)\n",
    "print(\"Accuracy on the test set:\",acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addb086e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flip all the images in the training set and expand them to the training set\n",
    "X_train_copy = X_train.tolist()\n",
    "y_train_copy = y_train.tolist()\n",
    "# print(X_train.shape)\n",
    "# print(y_train.shape)\n",
    "for i in range(len(X_train)):\n",
    "    temp_img = []\n",
    "    img = X_train[i].reshape(28,28)\n",
    "    img_new = cv2.flip(img, 0)  # Vertical Flip\n",
    "    img_new = img_new.reshape(-1).tolist()\n",
    "    X_train_copy.append(img_new)\n",
    "    y_train_copy.append(y_train[i])\n",
    "X_train = np.array(X_train_copy)\n",
    "y_train = np.array(y_train_copy)\n",
    "print(X_train.shape)  # Print the shape of x_train\n",
    "print(y_train.shape)  # Print the shape of y_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a652e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "You can see that the shape of both x_train and y_train become twice the original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58fae78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The optimal hyperparameter C, the regularization parameter, is selected using a Bayesian optimizer\n",
    "def f_score(C):\n",
    "    lr = LogisticRegression(max_iter=200,C=C)\n",
    "    lr.fit(X_train,y_train)\n",
    "    acc = accuracy_score(y_test,lr.predict(X_test))\n",
    "    return acc     # The goal of adjusting parameters to maximize accuracy\n",
    "\n",
    "# Determine the range of parameter values\n",
    "pbounds = {'C': (0,1)}\n",
    "\n",
    "# Constructing a Bayesian Optimizer\n",
    "opt = BayesianOptimization(\n",
    "    f=f_score, \n",
    "    pbounds=pbounds,  \n",
    "    verbose=2,  # verbose = 2 prints all, verbose = 1 prints the maximum value found in the run, verbose = 0 prints nothing\n",
    "    random_state=1\n",
    ")\n",
    "\n",
    "# Start running\n",
    "opt.maximize(\n",
    "    init_points=10,  # Steps of random search\n",
    "    n_iter=130  # Number of iterations to perform Bayesian optimization\n",
    ")\n",
    "\n",
    "C3 = opt.max['params'][\"C\"]     # Take the best parameters\n",
    "print(\"best C:\",C3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0860f46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training on the training set\n",
    "lr3 = LogisticRegression(max_iter=200,C=C3)\n",
    "lr3.fit(X_train,y_train)\n",
    "\n",
    "\n",
    "# Test accuracy in test set\n",
    "pre = lr3.predict(X_test)\n",
    "acc = accuracy_score(pre,y_test)\n",
    "print(\"Accuracy on the test set:\",acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b9939a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(lr1,\"./model/logistic_regression_classifier.m\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
